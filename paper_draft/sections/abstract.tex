\begin{abstract}
Belief representation claims in large language models are often supported by linear probing, but decodability alone does not establish semantic encoding. We test whether epistemic and non-epistemic belief labels localize in distinct layers and remain stable under wording changes. We run a unified layer-wise probing pipeline on \gpttwo, \gpttwomedium, and \llamasevenb across two tasks: \tomi (non-epistemic belief-state entailment) and \truthfulqa (epistemic truthfulness classification). For each layer, we train a balanced logistic probe on last-token hidden states and evaluate on original, lexical-perturbed, and rephrased-template test sets. We also compare against a \tfidf logistic baseline and run Wilcoxon tests with Benjamini--Hochberg correction.

We find clear layer-local decodable signal, especially for \truthfulqa: best independent test-layer \auroc reaches 0.692 (\gpttwo), 0.701 (\gpttwomedium), and 0.756 (\llamasevenb). However, validation-selected peak layers are sometimes misaligned with best test layers, and robustness patterns are mixed: several settings improve under perturbation while others degrade. No original-vs-perturbed difference is significant after \fdr correction ($p_{\mathrm{adj}}\geq 0.1875$). On \tomi, hidden-state probes are often close to or below the \tfidf baseline (0.568 \auroc), indicating substantial surface-feature contribution.

These results support a conservative interpretation: current probes provide useful diagnostics for where belief-related information is decodable, but they are insufficient evidence of stable, deep semantic belief representations.
\end{abstract}
