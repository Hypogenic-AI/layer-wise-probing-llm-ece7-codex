# Downloaded Papers

1. [Language Models Represent Beliefs of Self and Others](2024_language_models_represent_beliefs_of_self_and_others.pdf)
   - Authors: Wentao Zhu, Zhining Zhang, Yizhou Wang
   - Year: 2024
   - Source URL: https://arxiv.org/pdf/2402.18496.pdf
   - Status: downloaded
   - Local file: `papers/2024_language_models_represent_beliefs_of_self_and_others.pdf`
   - Why relevant: Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the...

2. [Representational and Behavioral Stability of Truth in Large Language Models](2025_representational_and_behavioral_stability_of_truth_in_large_language_models.pdf)
   - Authors: Samantha Dies, Courtney Maynard, Germans Savcisens, Tina Eliassi-Rad
   - Year: 2025
   - Source URL: https://arxiv.org/pdf/2511.19166.pdf
   - Status: downloaded
   - Local file: `papers/2025_representational_and_behavioral_stability_of_truth_in_large_language_models.pdf`
   - Why relevant: Large language models (LLMs) are increasingly used as information sources, yet small changes in semantic framing can destabilize their truth judgments. We propose P-StaT (Perturbation Stability of Truth), an evaluation...

3. [A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks](2025_a_survey_of_theory_of_mind_in_large_language_models_evaluations_representations_.pdf)
   - Authors: H. Nguyen
   - Year: 2025
   - Source URL: https://arxiv.org/pdf/2502.06470.pdf
   - Status: downloaded
   - Local file: `papers/2025_a_survey_of_theory_of_mind_in_large_language_models_evaluations_representations_.pdf`
   - Why relevant: Theory of Mind (ToM), the ability to attribute mental states to others and predict their behaviour, is fundamental to social intelligence. In this paper, we survey studies evaluating behavioural and representational ToM...

4. [Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models](2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua.pdf)
   - Authors: Matteo Bortoletto, Constantin Ruhdorfer, Lei Shi, Andreas Bulling
   - Year: 2024
   - Source URL: https://arxiv.org/pdf/2406.17513.pdf
   - Status: downloaded
   - Local file: `papers/2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua.pdf`
   - Why relevant: Despite growing interest in Theory of Mind (ToM) tasks for evaluating language models (LMs), little is known about how LMs internally represent mental states of self and others. Understanding these internal mechanisms...

5. [Truth-value judgment in language models:'truth directions'are context sensitive](https://api.semanticscholar.org/CorpusId:280137568)
   - Authors: Stefan F. Schouten, Peter Bloem, Ilia Markov, Piek Vossen
   - Year: 2024
   - Source URL: https://api.semanticscholar.org/CorpusId:280137568
   - Status: not_found
   - Why relevant: Recent work has demonstrated that the latent spaces of large language models (LLMs) contain directions predictive of the truth of sentences. Multiple methods recover such directions and build probes that are described...

6. [How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence](2025_how_post_training_reshapes_llms_a_mechanistic_view_on_knowledge_truthfulness_ref.pdf)
   - Authors: Hongzhe Du, Weikai Li, Min Cai, Karim Saraipour, Zimin Zhang, Himabindu Lakkaraju, Yizhou Sun, Shichang Zhang
   - Year: 2025
   - Source URL: https://arxiv.org/pdf/2504.02904.pdf
   - Status: downloaded
   - Local file: `papers/2025_how_post_training_reshapes_llms_a_mechanistic_view_on_knowledge_truthfulness_ref.pdf`
   - Why relevant: Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training...

7. [Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief](2025_enhancing_uncertainty_estimation_in_llms_with_expectation_of_aggregated_internal.pdf)
   - Authors: Zeguan Xiao, Diyang Dou, Boya Xiong, Yun Chen, Guanhua Chen
   - Year: 2025
   - Source URL: https://arxiv.org/pdf/2509.01564.pdf
   - Status: downloaded
   - Local file: `papers/2025_enhancing_uncertainty_estimation_in_llms_with_expectation_of_aggregated_internal.pdf`
   - Why relevant: Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, but often exhibit overconfidence and generate plausible yet incorrect answers. This overconfidence, especially...

8. [The Internal State of an LLM Knows When its Lying](2023_the_internal_state_of_an_llm_knows_when_its_lying.pdf)
   - Authors: A. Azaria, Tom M. Mitchell
   - Year: 2023
   - Source URL: https://aclanthology.org/2023.findings-emnlp.68.pdf
   - Status: downloaded
   - Local file: `papers/2023_the_internal_state_of_an_llm_knows_when_its_lying.pdf`
   - Why relevant: While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we...

9. [Still no lie detector for language models: probing empirical and conceptual roadblocks](2023_still_no_lie_detector_for_language_models_probing_empirical_and_conceptual_roadb.pdf)
   - Authors: B. A. Levinstein, Daniel A. Herrmann
   - Year: 2023
   - Source URL: https://arxiv.org/pdf/2307.00175.pdf
   - Status: downloaded
   - Local file: `papers/2023_still_no_lie_detector_for_language_models_probing_empirical_and_conceptual_roadb.pdf`
   - Why relevant: We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we consider whether or not we should expect LLMs to have something like beliefs...

10. [Improving Preference Extraction In LLMs By Identifying Latent Knowledge Through Classifying Probes](2025_improving_preference_extraction_in_llms_by_identifying_latent_knowledge_through_.pdf)
   - Authors: Sharan Maiya, Yinhong Liu, Ramit Debnath, Anna Korhonen
   - Year: 2025
   - Source URL: https://arxiv.org/pdf/2503.17755.pdf
   - Status: downloaded
   - Local file: `papers/2025_improving_preference_extraction_in_llms_by_identifying_latent_knowledge_through_.pdf`
   - Why relevant: Large Language Models (LLMs) are often used as automated judges to evaluate text, but their effectiveness can be hindered by various unintentional biases. We propose using linear classifying probes, trained by...

11. [Standards for Belief Representations in LLMs](2024_standards_for_belief_representations_in_llms.pdf)
   - Authors: Daniel A. Herrmann, B. A. Levinstein
   - Year: 2024
   - Source URL: https://arxiv.org/pdf/2405.21030.pdf
   - Status: downloaded
   - Local file: `papers/2024_standards_for_belief_representations_in_llms.pdf`
   - Why relevant: As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and...

12. [Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs](2025_mapping_clinical_doubt_locating_linguistic_uncertainty_in_llms.pdf)
   - Authors: Srivarshinee Sridhar, Raghav Kaushik Ravi, Kripabandhu Ghosh
   - Year: 2025
   - Source URL: https://arxiv.org/pdf/2511.22402.pdf
   - Status: downloaded
   - Local file: `papers/2025_mapping_clinical_doubt_locating_linguistic_uncertainty_in_llms.pdf`
   - Why relevant: Large Language Models (LLMs) are increasingly used in clinical settings, where sensitivity to linguistic uncertainty can influence diagnostic interpretation and decision-making. Yet little is known about where such...

13. [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](2023_a_survey_on_hallucination_in_large_language_models_principles_taxonomy_challenge.pdf)
   - Authors: Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu
   - Year: 2023
   - Source URL: https://arxiv.org/pdf/2311.05232.pdf
   - Status: downloaded
   - Local file: `papers/2023_a_survey_on_hallucination_in_large_language_models_principles_taxonomy_challenge.pdf`
   - Why relevant: The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to...

14. [The Trilemma of Truth in Large Language Models](2025_the_trilemma_of_truth_in_large_language_models.pdf)
   - Authors: Germans Savcisens, Tina Eliassi-Rad
   - Year: 2025
   - Source URL: https://arxiv.org/pdf/2506.23921.pdf
   - Status: downloaded
   - Local file: `papers/2025_the_trilemma_of_truth_in_large_language_models.pdf`
   - Why relevant: The public often attributes human-like qualities to large language models (LLMs) and assumes they"know"certain things. In reality, LLMs encode information retained during training as internal probabilistic knowledge....

15. [The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models'Posteriors](2026_the_shape_of_beliefs_geometry_dynamics_and_interventions_along_representation_ma.pdf)
   - Authors: Raphael Sarfati, Eric J. Bigelow, Daniel Wurgaft, Jack Merullo, Atticus Geiger, Owen Lewis, Tom McGrath, E. Lubana
   - Year: 2026
   - Source URL: https://arxiv.org/pdf/2602.02315.pdf
   - Status: downloaded
   - Local file: `papers/2026_the_shape_of_beliefs_geometry_dynamics_and_interventions_along_representation_ma.pdf`
   - Why relevant: Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with...

16. [Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models](2026_indications_of_belief_guided_agency_and_meta_cognitive_monitoring_in_large_langu.pdf)
   - Authors: Noam Steinmetz Yalon, Ariel Goldstein, L. Mudrik, Mor Geva
   - Year: 2026
   - Source URL: https://arxiv.org/pdf/2602.02467.pdf
   - Status: downloaded
   - Local file: `papers/2026_indications_of_belief_guided_agency_and_meta_cognitive_monitoring_in_large_langu.pdf`
   - Why relevant: Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for...

17. [On the Effect of Uncertainty on Layer-wise Inference Dynamics](2025_on_the_effect_of_uncertainty_on_layer_wise_inference_dynamics.pdf)
   - Authors: Sunwoo Kim, Haneul Yoo, Alice Oh
   - Year: 2025
   - Source URL: https://arxiv.org/pdf/2507.06722.pdf
   - Status: downloaded
   - Local file: `papers/2025_on_the_effect_of_uncertainty_on_layer_wise_inference_dynamics.pdf`
   - Why relevant: Understanding how large language models (LLMs) internally represent and process their predictions is central to detecting uncertainty and preventing hallucinations. While several studies have shown that models encode...

18. [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](2025_stable_but_miscalibrated_a_kantian_view_on_overconfidence_from_filters_to_large_.pdf)
   - Authors: Akira Okutomi
   - Year: 2025
   - Source URL: https://arxiv.org/pdf/2510.14925.pdf
   - Status: downloaded
   - Local file: `papers/2025_stable_but_miscalibrated_a_kantian_view_on_overconfidence_from_filters_to_large_.pdf`
   - Why relevant: We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition in...
