[
  {
    "title": "Language Models Represent Beliefs of Self and Others",
    "authors": "Wentao Zhu, Zhining Zhang, Yizhou Wang",
    "year": 2024,
    "relevance": 3,
    "citations": 16,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:268041304",
    "semantic_id": "CorpusId:268041304",
    "pdf_url": "https://arxiv.org/pdf/2402.18496.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2024_language_models_represent_beliefs_of_self_and_others.pdf",
    "status": "downloaded",
    "abstract": "Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations."
  },
  {
    "title": "Representational and Behavioral Stability of Truth in Large Language Models",
    "authors": "Samantha Dies, Courtney Maynard, Germans Savcisens, Tina Eliassi-Rad",
    "year": 2025,
    "relevance": 3,
    "citations": 0,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:283244203",
    "semantic_id": "CorpusId:283244203",
    "pdf_url": "https://arxiv.org/pdf/2511.19166.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2025_representational_and_behavioral_stability_of_truth_in_large_language_models.pdf",
    "status": "downloaded",
    "abstract": "Large language models (LLMs) are increasingly used as information sources, yet small changes in semantic framing can destabilize their truth judgments. We propose P-StaT (Perturbation Stability of Truth), an evaluation framework for testing belief stability under controlled semantic perturbations in representational and behavioral settings via probing and zero-shot prompting. Across sixteen open-source LLMs and three domains, we compare perturbations involving epistemically familiar Neither statements drawn from well-known fictional contexts (Fictional) to those involving unfamiliar Neither statements not seen in training data (Synthetic). We find a consistent stability hierarchy: Synthetic content aligns closely with factual representations and induces the largest retractions of previously held beliefs, producing up to $32.7\\%$ retractions in representational evaluations and up to $36.3\\%$ in behavioral evaluations. By contrast, Fictional content is more representationally distinct and comparatively stable. Together, these results suggest that epistemic familiarity is a robust signal across instantiations of belief stability under semantic reframing, complementing accuracy-based factuality evaluation with a notion of epistemic robustness."
  },
  {
    "title": "A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks",
    "authors": "H. Nguyen",
    "year": 2025,
    "relevance": 3,
    "citations": 5,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:276250539",
    "semantic_id": "CorpusId:276250539",
    "pdf_url": "https://arxiv.org/pdf/2502.06470.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2025_a_survey_of_theory_of_mind_in_large_language_models_evaluations_representations_.pdf",
    "status": "downloaded",
    "abstract": "Theory of Mind (ToM), the ability to attribute mental states to others and predict their behaviour, is fundamental to social intelligence. In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models (LLMs), identify important safety risks from advanced LLM ToM capabilities, and suggest several research directions for effective evaluation and mitigation of these risks."
  },
  {
    "title": "Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models",
    "authors": "Matteo Bortoletto, Constantin Ruhdorfer, Lei Shi, Andreas Bulling",
    "year": 2024,
    "relevance": 3,
    "citations": 4,
    "paper_finder_url": "https://www.semanticscholar.org/paper/d6bd903ccfa66a08c73a8c01d54d4d133246d6fe",
    "semantic_id": "d6bd903ccfa66a08c73a8c01d54d4d133246d6fe",
    "pdf_url": "https://arxiv.org/pdf/2406.17513.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua.pdf",
    "status": "downloaded",
    "abstract": "Despite growing interest in Theory of Mind (ToM) tasks for evaluating language models (LMs), little is known about how LMs internally represent mental states of self and others. Understanding these internal mechanisms is critical - not only to move beyond surface-level performance, but also for model alignment and safety, where subtle misattributions of mental states may go undetected in generated outputs. In this work, we present the first systematic investigation of belief representations in LMs by probing models across different scales, training regimens, and prompts - using control tasks to rule out confounds. Our experiments provide evidence that both model size and fine-tuning substantially improve LMs' internal representations of others' beliefs, which are structured - not mere by-products of spurious correlations - yet brittle to prompt variations. Crucially, we show that these representations can be strengthened: targeted edits to model activations can correct wrong ToM inferences."
  },
  {
    "title": "Truth-value judgment in language models:'truth directions'are context sensitive",
    "authors": "Stefan F. Schouten, Peter Bloem, Ilia Markov, Piek Vossen",
    "year": 2024,
    "relevance": 3,
    "citations": 2,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:280137568",
    "semantic_id": "CorpusId:280137568",
    "pdf_url": "",
    "source": "",
    "file": "",
    "status": "not_found",
    "abstract": "Recent work has demonstrated that the latent spaces of large language models (LLMs) contain directions predictive of the truth of sentences. Multiple methods recover such directions and build probes that are described as uncovering a model's\"knowledge\"or\"beliefs\". We investigate this phenomenon, looking closely at the impact of context on the probes. Our experiments establish where in the LLM the probe's predictions are (most) sensitive to the presence of related sentences, and how to best characterize this kind of sensitivity. We do so by measuring different types of consistency errors that occur after probing an LLM whose inputs consist of hypotheses preceded by (negated) supporting and contradicting sentences. We also perform a causal intervention experiment, investigating whether moving the representation of a premise along these truth-value directions influences the position of an entailed or contradicted sentence along that same direction. We find that the probes we test are generally context sensitive, but that contexts which should not affect the truth often still impact the probe outputs. Our experiments show that the type of errors depend on the layer, the model, and the kind of data. Finally, our results suggest that truth-value directions are causal mediators in the inference process that incorporates in-context information."
  },
  {
    "title": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence",
    "authors": "Hongzhe Du, Weikai Li, Min Cai, Karim Saraipour, Zimin Zhang, Himabindu Lakkaraju, Yizhou Sun, Shichang Zhang",
    "year": 2025,
    "relevance": 3,
    "citations": 5,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:277596203",
    "semantic_id": "CorpusId:277596203",
    "pdf_url": "https://arxiv.org/pdf/2504.02904.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2025_how_post_training_reshapes_llms_a_mechanistic_view_on_knowledge_truthfulness_ref.pdf",
    "status": "downloaded",
    "abstract": "Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training. Our code is publicly available at https://github.com/HZD01/post-training-mechanistic-analysis."
  },
  {
    "title": "Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief",
    "authors": "Zeguan Xiao, Diyang Dou, Boya Xiong, Yun Chen, Guanhua Chen",
    "year": 2025,
    "relevance": 3,
    "citations": 0,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:281080475",
    "semantic_id": "CorpusId:281080475",
    "pdf_url": "https://arxiv.org/pdf/2509.01564.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2025_enhancing_uncertainty_estimation_in_llms_with_expectation_of_aggregated_internal.pdf",
    "status": "downloaded",
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, but often exhibit overconfidence and generate plausible yet incorrect answers. This overconfidence, especially in models undergone Reinforcement Learning from Human Feedback (RLHF), poses significant challenges for reliable uncertainty estimation and safe deployment. In this paper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel self-evaluation-based calibration method that leverages the internal hidden states of LLMs to derive more accurate confidence scores. Instead of relying on the model's final output, our approach extracts internal beliefs from multiple intermediate layers during self-evaluation. By aggregating these layer-wise beliefs and calculating the expectation over the resulting confidence score distribution, EAGLE produces a refined confidence score that more faithfully reflects the model's internal certainty. Extensive experiments on diverse datasets and LLMs demonstrate that EAGLE significantly improves calibration performance over existing baselines. We also provide an in-depth analysis of EAGLE, including a layer-wise examination of uncertainty patterns, a study of the impact of self-evaluation prompts, and an analysis of the effect of self-evaluation score range."
  },
  {
    "title": "The Internal State of an LLM Knows When its Lying",
    "authors": "A. Azaria, Tom M. Mitchell",
    "year": 2023,
    "relevance": 2,
    "citations": 527,
    "paper_finder_url": "https://www.semanticscholar.org/paper/f406aceba4f29cc7cfbe7edb2f52f01374486589",
    "semantic_id": "f406aceba4f29cc7cfbe7edb2f52f01374486589",
    "pdf_url": "https://aclanthology.org/2023.findings-emnlp.68.pdf",
    "source": "semantic_open_access",
    "file": "papers/2023_the_internal_state_of_an_llm_knows_when_its_lying.pdf",
    "status": "downloaded",
    "abstract": "While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\\% to 83\\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios."
  },
  {
    "title": "Still no lie detector for language models: probing empirical and conceptual roadblocks",
    "authors": "B. A. Levinstein, Daniel A. Herrmann",
    "year": 2023,
    "relevance": 2,
    "citations": 85,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:259316868",
    "semantic_id": "CorpusId:259316868",
    "pdf_url": "https://arxiv.org/pdf/2307.00175.pdf",
    "source": "arxiv_title_match",
    "file": "papers/2023_still_no_lie_detector_for_language_models_probing_empirical_and_conceptual_roadb.pdf",
    "status": "downloaded",
    "abstract": "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. With this lesson in hand, we evaluate two existing approaches for measuring the beliefs of LLMs, one due to Azaria and Mitchell (The internal state of an llm knows when its lying, 2023) and the other to Burns et al. (Discovering latent knowledge in language models without supervision, 2022). Moving from the armchair to the desk chair, we provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. We conclude by suggesting some concrete paths for future work."
  },
  {
    "title": "Improving Preference Extraction In LLMs By Identifying Latent Knowledge Through Classifying Probes",
    "authors": "Sharan Maiya, Yinhong Liu, Ramit Debnath, Anna Korhonen",
    "year": 2025,
    "relevance": 2,
    "citations": 4,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:277272018",
    "semantic_id": "CorpusId:277272018",
    "pdf_url": "https://arxiv.org/pdf/2503.17755.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2025_improving_preference_extraction_in_llms_by_identifying_latent_knowledge_through_.pdf",
    "status": "downloaded",
    "abstract": "Large Language Models (LLMs) are often used as automated judges to evaluate text, but their effectiveness can be hindered by various unintentional biases. We propose using linear classifying probes, trained by leveraging differences between contrasting pairs of prompts, to directly access LLMs' latent knowledge and extract more accurate preferences. Through extensive experiments using models of varying size from four different families and six diverse datasets assessing text quality evaluation and common sense reasoning, we demonstrate that both supervised and unsupervised probing approaches consistently outperform traditional generation-based judgement while maintaining similar computational costs. These probes generalise under domain shifts and can even outperform finetuned evaluators with the same training data size. Our results suggest linear probing offers an accurate, robust and computationally efficient approach for LLM-as-judge tasks while providing interpretable insights into how models encode judgement-relevant knowledge. Our data and code will be openly released in the future."
  },
  {
    "title": "Standards for Belief Representations in LLMs",
    "authors": "Daniel A. Herrmann, B. A. Levinstein",
    "year": 2024,
    "relevance": 2,
    "citations": 26,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:270199407",
    "semantic_id": "CorpusId:270199407",
    "pdf_url": "https://arxiv.org/pdf/2405.21030.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2024_standards_for_belief_representations_in_llms.pdf",
    "status": "downloaded",
    "abstract": "As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world. However, this field currently lacks a unified theoretical foundation to underpin the study of belief in LLMs. This article begins filling this gap by proposing adequacy conditions for a representation in an LLM to count as belief-like. We argue that, while the project of belief measurement in LLMs shares striking features with belief measurement as carried out in decision theory and formal epistemology, it also differs in ways that should change how we measure belief. Thus, drawing from insights in philosophy and contemporary practices of machine learning, we establish four criteria that balance theoretical considerations with practical constraints. Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs. We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations."
  },
  {
    "title": "Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs",
    "authors": "Srivarshinee Sridhar, Raghav Kaushik Ravi, Kripabandhu Ghosh",
    "year": 2025,
    "relevance": 2,
    "citations": 0,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:283438747",
    "semantic_id": "CorpusId:283438747",
    "pdf_url": "https://arxiv.org/pdf/2511.22402.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2025_mapping_clinical_doubt_locating_linguistic_uncertainty_in_llms.pdf",
    "status": "downloaded",
    "abstract": "Large Language Models (LLMs) are increasingly used in clinical settings, where sensitivity to linguistic uncertainty can influence diagnostic interpretation and decision-making. Yet little is known about where such epistemic cues are internally represented within these models. Distinct from uncertainty quantification, which measures output confidence, this work examines input-side representational sensitivity to linguistic uncertainty in medical text. We curate a contrastive dataset of clinical statements varying in epistemic modality (e.g.,'is consistent with'vs.'may be consistent with') and propose Model Sensitivity to Uncertainty (MSU), a layerwise probing metric that quantifies activation-level shifts induced by uncertainty cues. Our results show that LLMs exhibit structured, depth-dependent sensitivity to clinical uncertainty, suggesting that epistemic information is progressively encoded in deeper layers. These findings reveal how linguistic uncertainty is internally represented in LLMs, offering insight into their interpretability and epistemic reliability."
  },
  {
    "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
    "authors": "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu",
    "year": 2023,
    "relevance": 2,
    "citations": 2142,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:265067168",
    "semantic_id": "CorpusId:265067168",
    "pdf_url": "https://arxiv.org/pdf/2311.05232.pdf",
    "source": "arxiv_title_match",
    "file": "papers/2023_a_survey_on_hallucination_in_large_language_models_principles_taxonomy_challenge.pdf",
    "status": "downloaded",
    "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations."
  },
  {
    "title": "The Trilemma of Truth in Large Language Models",
    "authors": "Germans Savcisens, Tina Eliassi-Rad",
    "year": 2025,
    "relevance": 2,
    "citations": 2,
    "paper_finder_url": "https://www.semanticscholar.org/paper/995faf2c6d7443bc24e3fc340867f392137785e6",
    "semantic_id": "995faf2c6d7443bc24e3fc340867f392137785e6",
    "pdf_url": "https://arxiv.org/pdf/2506.23921.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2025_the_trilemma_of_truth_in_large_language_models.pdf",
    "status": "downloaded",
    "abstract": "The public often attributes human-like qualities to large language models (LLMs) and assumes they\"know\"certain things. In reality, LLMs encode information retained during training as internal probabilistic knowledge. This study examines existing methods for probing the veracity of that knowledge and identifies several flawed underlying assumptions. To address these flaws, we introduce sAwMIL (Sparse-Aware Multiple-Instance Learning), a multiclass probing framework that combines multiple-instance learning with conformal prediction. sAwMIL leverages internal activations of LLMs to classify statements as true, false, or neither. We evaluate sAwMIL across 16 open-source LLMs, including default and chat-based variants, on three new curated datasets. Our results show that (1) common probing methods fail to provide a reliable and transferable veracity direction and, in some settings, perform worse than zero-shot prompting; (2) truth and falsehood are not encoded symmetrically; and (3) LLMs encode a third type of signal that is distinct from both true and false."
  },
  {
    "title": "The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models'Posteriors",
    "authors": "Raphael Sarfati, Eric J. Bigelow, Daniel Wurgaft, Jack Merullo, Atticus Geiger, Owen Lewis, Tom McGrath, E. Lubana",
    "year": 2026,
    "relevance": 2,
    "citations": 0,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:285270894",
    "semantic_id": "CorpusId:285270894",
    "pdf_url": "https://arxiv.org/pdf/2602.02315.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2026_the_shape_of_beliefs_geometry_dynamics_and_interventions_along_representation_ma.pdf",
    "status": "downloaded",
    "abstract": "Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved\"belief manifolds\"for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction."
  },
  {
    "title": "Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models",
    "authors": "Noam Steinmetz Yalon, Ariel Goldstein, L. Mudrik, Mor Geva",
    "year": 2026,
    "relevance": 2,
    "citations": 0,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:285271259",
    "semantic_id": "CorpusId:285271259",
    "pdf_url": "https://arxiv.org/pdf/2602.02467.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2026_indications_of_belief_guided_agency_and_meta_cognitive_monitoring_in_large_langu.pdf",
    "status": "downloaded",
    "abstract": "Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs."
  },
  {
    "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics",
    "authors": "Sunwoo Kim, Haneul Yoo, Alice Oh",
    "year": 2025,
    "relevance": 2,
    "citations": 2,
    "paper_finder_url": "https://api.semanticscholar.org/CorpusId:280138089",
    "semantic_id": "CorpusId:280138089",
    "pdf_url": "https://arxiv.org/pdf/2507.06722.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2025_on_the_effect_of_uncertainty_on_layer_wise_inference_dynamics.pdf",
    "status": "downloaded",
    "abstract": "Understanding how large language models (LLMs) internally represent and process their predictions is central to detecting uncertainty and preventing hallucinations. While several studies have shown that models encode uncertainty in their hidden states, it is underexplored how this affects the way they process such hidden states. In this work, we demonstrate that the dynamics of output token probabilities across layers for certain and uncertain outputs are largely aligned, revealing that uncertainty does not seem to affect inference dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to analyze the layer-wise probability trajectories of final prediction tokens across 11 datasets and 5 models. Using incorrect predictions as those with higher epistemic uncertainty, our results show aligned trajectories for certain and uncertain predictions that both observe abrupt increases in confidence at similar layers. We balance this finding by showing evidence that more competent models may learn to process uncertainty differently. Our findings challenge the feasibility of leveraging simplistic methods for detecting uncertainty at inference. More broadly, our work demonstrates how interpretability methods may be used to investigate the way uncertainty affects inference."
  },
  {
    "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models",
    "authors": "Akira Okutomi",
    "year": 2025,
    "relevance": 2,
    "citations": 0,
    "paper_finder_url": "https://www.semanticscholar.org/paper/417d6f97c3061493dd54e43cb916b043523e0825",
    "semantic_id": "417d6f97c3061493dd54e43cb916b043523e0825",
    "pdf_url": "https://arxiv.org/pdf/2510.14925.pdf",
    "source": "arxiv_externalid",
    "file": "papers/2025_stable_but_miscalibrated_a_kantian_view_on_overconfidence_from_filters_to_large_.pdf",
    "status": "downloaded",
    "abstract": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition in linear-Gaussian state-space models via H-Risk, a composite instability index integrating spectral margin, conditioning, temporal sensitivity, and innovation amplification. In simulations, higher H-Risk predicts overconfident errors and degraded closed-loop behavior even when the dynamics remain formally stable, exposing a gap between nominal and epistemic stability. Extending this stability lens to large language models (LLMs), we introduce a domain-wise proxy based on confidence fluctuations and overconfident errors. In a binary-question study, a Kantian-inspired policy that permits''cannot judge''responses yields targeted reductions in policy-aware squared loss in high-stakes domains relative to an overconfident baseline. To probe internal dynamics, we analyse layer-wise sensitivity of hidden states to small input perturbations. Contrary to a naive instability hypothesis, confidently wrong answers show no instability gap; instead, they are at least as locally stable as confidently correct answers, revealing stable miscalibration in which hallucinations behave like robust but misaligned attractors. For Qwen-2.5, spectral and activation profiles suggest a high signal-to-noise, low effective signal temperature regime in which representations become inertial and resistant to contextual shifts. These results bridge Kantian self-limitation and feedback control, and suggest that stable high-confidence hallucinations may not be readily corrected by output-only heuristics (e.g., temperature scaling or re-sampling), motivating process-level interventions that explicitly perturb and re-evaluate the inference trajectory."
  }
]