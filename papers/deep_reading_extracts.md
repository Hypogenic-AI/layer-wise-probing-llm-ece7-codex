# Deep Reading Chunk Notes

## 2024_language_models_represent_beliefs_of_self_and_others_chunk_*.pdf

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_001.pdf

Language Models Represent Beliefs of Self and Others Wentao Zhu 1 Zhining Zhang 1 Yizhou Wang1 2 3 4 Abstract Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social rea- soning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mech- anisms underlying these capabilities remain elu- sive. In this study, we discover that it is possible to linearly decode the belief status from the per- spectives of various agents through neural activa- tions of language models, indicating the existence of internal representations of self and others’ be- liefs. By manipulating these representations, we observe dramatic changes in the models’ ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations. * 1. Introduction Developing machine systems that can engage in sophisti- cated social reasoning in a human-like manner represents one of the paramount goals in artificial intelligence. At the core of such an endeavor is the necessity for these systems to possess a “Theory of Mind” (ToM) capability, which in- volves recognizing and attributing mental states — such as beliefs, desires, intentions, and emotions — to oneself and others, while acknowledging that others may possess men- tal states distinct from one’s own (Leslie, 1987; Wellman et al., 2001). This foundational capability is crucial not only 1Center on Frontiers of Computing Studies, School of Compter Science, Peking University 2Inst. for Artificial In- telligence, Peking University 3Nat’l Eng. Research Center of Visual Technology, Peking University 4Nat’l Key Lab of General Artificial Intelligence, Peking University. Correspon- dence to: Wentao Zhu <wtzhu@pku.edu.cn>, Yizhou Wang <yizhou.wang@pku.edu.cn>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). *Project page: https://walter0807.github.io/ RepBelief/ for the nuanced navigati

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_002.pdf

Language Models Represent Beliefs of Self and Others (A) Oracle (B) Protagonist (C) Joint Figure 2. Probe accuracies based on the attention head activations in all layers of Mistral-7B. (A) Belief status estimation for oracle using logistic regression (binary). (B) Belief status estimation for protagonist using logistic regression (binary). (C) Joint belief status estimation for both agents using multinomial logistic regression (quaternary). (A) Layer 4, Head 30 (B) Layer 10, Head 26 (C) Layer 10, Head 26 Figure 3. Illustration of linear separability of the belief representations. We show visual explanations for typical representation spaces: In (A), oracle belief status can be precisely estimated with a linear model, while protagonist cannot. The red and blue lines represent linear decision boundaries for oracle and protagonist, respectively; In (B), the belief statuses of both oracle and protagonist can be accurately modeled using linear models; (C) further shows the decision boundaries for joint belief status estimation using multinomial linear regression model, with arrows indicating the probing weight directions for each class. better than random guess. However, it is worth noting that a specific group of attention heads in the middle layers ex- hibits remarkably better performance, achieving over 80% validation accuracy. This phenomenon suggests that these attention heads implicitly encode the belief status of other agents in a linearly-decodable way. Multinomial Probing. We further explore the possibility of estimating the belief statuses of both agents from the activations simultaneously. For this purpose, we combine the belief labels of both agents into a four-dimensional vari- able representing their joint belief statuses using one-hot encoding, where each dimension corresponds to a unique combination of yo and yp. For brevity, we define this joint belief variable as ym ∈ {0, 1}N ×4. A multinomial logistic regression model is utilized to predict the joint belief ˆym. Specifically, the class probabilities are derived by applying the softmax function to the linear transformations of x: ˆym = softmax(xWm + bm), (3) where Wm ∈ RD×4 is the weight matrix for

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_003.pdf

Language Models Represent Beliefs of Self and Others Forward Belief Acc. Backward Belief Acc. Correlation = 0.90 Forward Belief Acc. Forward Action Acc. Correlation = 0.89 Forward Action Acc. Backward Belief Acc. Correlation = 0.85 Figure 6. Pairwise comparisons of multinomial probing results on Mistral-7B. Each point represents a specific attention head. The point position denotes its probe accuracies in the two tasks, and point color denotes the cosine similarity between the (+ TpFo) probe directions of the two tasks. Prompt: … However, a sudden storm appears on the horizon, bringing rough seas and strong winds. Kofi does not notice the approaching storm. Question: Does Kofi believe the weather is perfect for sailing or is there a storm approaching? Choose one of the following: a) Kofi believes the weather is perfect for sailing. b) Kofi believes there is a storm approaching. Answer: Figure 7. (Left) Question prompt of the False Belief condition for the Forward Belief task. The story background is omitted for simplicity. (Right) Changes of next-token probability with regard to different intervention strength α on Mistral-7B. corresponding to the rightwards direction vertical to the red boundary in Figure 3 (B)) • Weight directions for joint belief status prediction, de- rived from multinomial probing. We focus on the di- rection which maximizes the probability of recogniz- ing protagonist’s belief as True and meanwhile distin- guishes it from oracle’s belief (+TpFo, corresponding to the blue arrow in Figure 3 (C)). For random directions, we use the top-K informative heads identified with multinomial probing. For different social reasoning tasks, we separately probe the attention head acti- vations by constructing the corresponding prompt templates. For example, for the Forward Action task, we utilize the story-action pairs, while for theBackward Belief task, we in- clude the protagonist’s next-step actions in the story. More details can be found in Appendix B. 4.2.2. R ESULTS . We evaluate different activation intervention strategies and present the results in Table 1. For different tasks, we per- form intervention based on their respective probing results. Th

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_004.pdf

Language Models Represent Beliefs of Self and Others References Alain, G. and Bengio, Y . Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016. Baker, C. L., Saxe, R., and Tenenbaum, J. B. Action under- standing as inverse planning. Cognition, 113(3):329–349, 2009. Baker, C. L., Jara-Ettinger, J., Saxe, R., and Tenenbaum, J. B. Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. Nature Human Be- haviour, 1(4):0064, 2017. Baron-Cohen, S., Leslie, A. M., and Frith, U. Does the autistic child have a “theory of mind”? Cognition, 21(1): 37–46, 1985. Bau, D., Zhu, J.-Y ., Strobelt, H., Lapedriza, A., Zhou, B., and Torralba, A. Understanding the role of individual units in a deep neural network. Proceedings of the Na- tional Academy of Sciences, 2020. Belinkov, Y . Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207–219, 2022. Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and trans- parency, pp. 610–623, 2021. Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. Bubeck, S., Chandrasekaran, V ., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y . T., Li, Y ., Lundberg, S., et al. Sparks of artificial general intel- ligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Burns, C., Ye, H., Klein, D., and Steinhardt, J. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022. D¨ohnel, K., Schuwerk, T., Meinhardt, J., Sodian, B., Hajak, G., and Sommer, M. Functional activity of the right temporo-parietal junction and of the medial prefrontal cortex associated with true and false belief reasoning. Neuroimage, 60(3):1652–1661, 2012. Dolan, B. and Brockett, C. Automatically constructing a corpus of sentential paraphrases. In Third international workshop 

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_005.pdf

Language Models Represent Beliefs of Self and Others A. Statistical Analysis on Belief Probing To further verify the existence of attention heads that encode belief status of different agents, we conduct robust statistical analysis. Specifically, we train and evaluate the linear probes across different data splits using 100 random seeds. We report the mean accuracies and 95% confidence intervals (CI) for the top attention heads in Tables 3 to 5. We calculate the validation accuracy of linear probes for both True Belief (TB) and False Belief (FB) scenarios respectively. The probes are trained with full training set with both TB and FB. Furthermore, we apply a multiple hypothesis testing correction for our analysis of the top-10 attention heads. We adopt the stringent Bonferroni correction method that rigorously control the Family-Wise Error Rate (FWER), thereby significantly reducing the likelihood of type I errors. Under this framework, each attention head is scrutinized against a null hypothesis positing that its accuracy does not surpass a specific baseline (75%), versus an alternative hypothesis that asserts superior accuracy. The results robustly validates that a specific subset of attention heads exhibits the capacity to predict agents’ belief states with a validation accuracy exceeding 75%. Significantly low p-values with the Bonferroni correction strongly reject the null hypothesis, affirming the statistical significance and reliability of our findings. Table 3. The top predictive attention heads in belief status estimation for oracle using logistic regression (binary). We use Mistral-7B model in the Forward Belief task. Random guessing is 50%. Position Mean Acc. CI Corrected P-Value Mean Acc (TB) CI (TB) Mean Acc (FB) CI (FB) (14, 31) 97.8 (97.5, 98.1) 0.0000 97.7 (97.3, 98.2) 97.8 (97.4, 98.2) (13, 26) 97.5 (97.1, 97.8) 0.0000 97.1 (96.6, 97.5) 97.8 (97.4, 98.2) (14, 11) 97.5 (97.1, 97.8) 0.0000 97.5 (97.0, 98.0) 97.4 (97.0, 97.8) (14, 18) 97.4 (97.0, 97.7) 0.0000 97.6 (97.1, 98.0) 97.1 (96.7, 97.6) (14, 8) 97.3 (96.9, 97.6) 0.0000 97.1 (96.6, 97.6) 97.4 (97.0, 97.8) (14, 26) 97.2 (96.9, 97.6) 0.0000 96.8 (96.3, 97.3) 97.6 (97.2, 98.1) (15, 23) 97.2 (96

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_006.pdf

Language Models Represent Beliefs of Self and Others (A) Oracle (B) Protagonist (C) Joint DeepSeek-7B Forward Belief Backward Belief Forward Action Figure 12. Probe accuracies on different tasks based on the attention head activations in all layers of DeepSeek-7B. (A) Belief status estimation for oracle using logistic regression (binary). (B) Belief status estimation for protagonist using logistic regression (binary). (C) Joint belief status estimation for both agents using multinomial logistic regression (quaternary). 16 Language Models Represent Beliefs of Self and Others (A) Oracle (B) Protagonist (C) Joint Mistral-7B (non-linear probing) Forward Belief Backward Belief Forward Action Figure 13. Non-linear probe accuracies on different tasks based on the attention head activations in all layers of Mistral-7B. (A) Belief status estimation for oracle using binary classification. (B) Belief status estimation for protagonist using binary classification. (C) Joint belief status estimation for both agents using quaternary classification. 17 Language Models Represent Beliefs of Self and Others D. Generalization to Other Datasets In addition to the stories in BigToM (Gandhi et al., 2023), we explore whether our findings could generalize to other narratives. Following (Wilf et al., 2023), we extend our study to the ToMi benchmark (Le et al., 2019), which has quite different narrative templates and scenarios compared to BigToM. It also contains second-order ToM questions which are not present in BigToM. (A) Oracle (B) Protagonist (C) Joint Figure 14. Probe accuracies on ToMi dataset based on the attention head activations in all layers of DeepSeek-7B. (A) Belief status estimation for oracle using logistic regression (binary). (B) Belief status estimation for protagonist using logistic regression (binary). (C) Joint belief status estimation for both agents using multinomial logistic regression (quaternary). We perform probing on ToMi by constructing the prompt templates based on its question format, as shown below. Since the ToMi questions are predicting the protagonists’ next-step move, it is closer to theForward Actiontask in BigToM. Listing 1. An example of the prompt

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_007.pdf

Language Models Represent Beliefs of Self and Others Acc. (BigToM, Forward Action) Acc. (ToMi) Correlation = 0.86 Figure 15. Pairwise comparisons of multinomial probing results on Mistral-7B between ToMi and BigToM. Each point represents a specific attention head, with its position denoting the probe accuracies in the two datasets (multinomial), and its color denoting the cosine similarity between the (+TpFo) probe directions of the two datasets.. E. Influence on Other Tasks Table 7. Model performance comparison on the MMLU benchmark. All Humanities Social Sciences STEM Other Mistral-7B (baseline) 57.5 52.7 66.4 48.3 65.5 Mistral-7B (+random) 57.6 52.7 66.3 48.4 65.6 Mistral-7B (+TpFo) 57.7 53.0 66.3 48.7 65.5 Table 8. Model performance comparison on CoLA, MRPC, and QNLI benchmarks. Dataset CoLA MRPC QNLI Mistral-7B (baseline) 70.0 58.8 62.5 Mistral-7B (+random) 70.1 59.2 63.1 Mistral-7B (+TpFo) 71.1 58.9 63.0 In order to understand the influence of intervention along the identified belief directions on unrelated tasks, we further evaluate the model on some general language understanding benchmarks, including Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020), The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019), Microsoft Research Paraphrase Corpus (MRPC) (Dolan & Brockett, 2005), Question-answering NLI (QNLI) (Wang et al., 2018). These benchmarks measure the language model’s performance in tasks unrelated to ToM, including knowledge acquicision, grammar check, etc. We discover that the activation intervention along the identified ToM directions does not significantly change the model performance, as shown in Tables 7 and 8. F. Additional Token Gradient Heatmaps We provide additional results of token gradients with regard to the belief directions given different story prompts, which localizes the key causal elements related to agent beliefs. 19 Language Models Represent Beliefs of Self and Others <s> R avi is a farmer in a small Indian village , t ending to his m ango or ch ard . R avi wants to harvest ri pe m ang oes to sell at the local market . He not ices a m ango tree with a heavy branch l aden with ri pe m ang oes . 

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_008.pdf

Language Models Represent Beliefs of Self and Others G. Hyperparameter Analysis True Belief False Belief True Belief False Belief Figure 18. Impact of varying intervention strength α on the Forward Belief task using Mistral-7B. “Invalid” denotes the answer is not recognized by the grading mechanism as the model fails to provide answer in the required format, e.g., delivering uncertain responses. We provide intervention results with different(K, α) combinations in Figure 18. K = 16 yields the most steady performance and is used in our experiments for the model. 22 Language Models Represent Beliefs of Self and Others H. Detailed Comparison of LLM Responses Below, we compare the performance of two models (Mistral-7B, DeepSeek-7B) on the Forward Belief reasoning task before and after intervention in two settings (False Belief, True Belief). Instances where the model’s response changes from incorrect to correct, or from correct to incorrect, are recorded in the tables below. H.1. Mistral-7B on Forward Belief, False Belief Changed to Correct Answer: 33/100 Story: Ravi is a farmer in a small Indian village, tending to his mango orchard. Ravi wants to harvest ripe mangoes to sell at the local market. He notices a mango tree with a heavy branch laden with ripe mangoes. A mischievous monkey comes along and eats all the ripe mangoes on the branch while Ravi is gathering his harvesting tools. Ravi does not see the monkey eating the ripe mangoes. Question: Does Ravi believe the branch has ripe mangoes or no ripe mangoes? Choose one of the following: a) Ravi believes the branch has ripe mangoes. b) Ravi believes the branch has no ripe mangoes. Correct Answer: a) Unintervened: b) Ravi believes the branch has no ripe mangoes. Intervened: a. Ravi believes the branch has ripe mangoes. Story: Aniket is a marine biologist studying coral reefs off the coast of India. Aniket needs to collect samples of coral to analyze the effects of climate change on the reef. Aniket spots a healthy-looking coral formation in a specific area of the reef. A sudden wave surge stirs up sediment, covering the once healthy coral formation and causing it to become damaged. Aniket does not notice the wave 

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_009.pdf

Language Models Represent Beliefs of Self and Others Story: Carlos is a farmer in a small village in the Andes. Carlos wants to collect fresh eggs from his chickens to sell at the local market. The nest appears to be full of eggs when Carlos checks it in the morning. A crafty fox sneaks into the chicken coop and steals all the eggs from the nest. Carlos does not see the fox stealing the eggs. Question: Does Carlos believe the nest contains eggs or that the fox has stolen them? Choose one of the following: a) Carlos believes the fox has stolen the eggs. b) Carlos believes the nest contains eggs. Correct Answer: b) Unintervened: a) Carlos believes the nest contains eggs. Intervened: b) Carlos believes the nest contains eggs. (at the time of asking) Story: Amara is an artist living in a bustling city in Nigeria. She wants to create a beautiful painting for her friend’s birthday present. She finds a smooth, flawless canvas in her art studio. Later, her mischievous little brother accidentally spills black paint all over the canvas, leaving it ruined. Amara does not discover the ruined canvas. Question: Does Amara believe the canvas is smooth and flawless or ruined with black paint? Choose one of the following: a) Amara believes the canvas is ruined with black paint. b) Amara believes the canvas is smooth and flawless. Correct Answer: b) Unintervened: a) Amara believes the canvas is smooth and flawless (but it is actually ruined). Intervened: Amara believes the canvas is smooth and flawless. (At the time of the story, she Story: Fatima is a talented artist living in a small Moroccan village. Fatima wants to create a beautiful painting to donate for the local charity auction. She has a brand-new set of high-quality paintbrushes in her studio. While she is away, her younger brother accidentally ruins the paintbrushes by using them for a school project with glue. Fatima does not notice the ruined paintbrushes. Question: Does Fatima believe she has high-quality paintbrushes or ruined paintbrushes in her studio? Choose one of the following: a) Fatima believes the paintbrushes are ruined. b) Fatima believes she has high-quality paintbrushes in her studio. Correct Answer: b)

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_010.pdf

Language Models Represent Beliefs of Self and Others Story: Emilia is a talented baker in a small Italian town. She plans to bake a large batch of biscotti to sell at the local market. Emilia inspects the oven’s temperature, which appears to be set at the correct temperature for baking. A power surge causes the oven’s temperature to spike, making it too hot for baking biscotti. Emilia does not notice the oven’s temperature increase. Question: Does Emilia believe the oven is set at the correct temperature for baking biscotti or too hot? Choose one of the following: a) Emilia believes the oven is set at the correct temperature for baking biscotti. b) Emilia believes the oven is too hot for baking biscotti. Correct Answer: a) Unintervened: b) Emilia believes the oven is too hot for baking biscotti (even though she doesn Intervened: a) Emilia believes the oven is set at the correct temperature for baking biscotti. ( Story: Emeka is a skilled fisherman in a small Nigerian village. He wants to catch fish to feed his family and sell at the market. Emeka prepares his fishing net, which appears to be strong and without any holes. However, while Emeka is not looking, a mischievous child cuts a large hole in the net with a knife. Emeka does not see the child cutting the hole in the fishing net. Question: Does Emeka believe the fishing net is strong and without holes, or that it has a large hole in it? Choose one of the following: a) Emeka believes the fishing net is strong and without holes. b) Emeka believes that the fishing net has a large hole in it. Correct Answer: a) Unintervened: b) Emeka believes that the fishing net has a large hole in it (without knowing it Intervened: a) Emeka believes the fishing net is strong and without holes. Story: Aria is a talented baker from Italy who has been asked to bake a cake for a friend’s birthday party. She wants to create a delicious, multi-layered cake that will impress everyone. Aria has a fresh batch of ingredients ready to use for the cake. However, a mischievous cat sneaks into her kitchen and nibbles on some of the ingredients, making them no longer fresh. Aria does not notice the cat or the nibbled ingredients. Question: D

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_011.pdf

Language Models Represent Beliefs of Self and Others Story: Chi is a fisherman living on a small island in the South Pacific. Chi wants to go on a fishing trip to catch some fish for his family’s dinner. The sea appears calm and inviting. Suddenly, a strong storm rolls in, causing the sea to become rough and dangerous. Chi notices the storm and the change in the sea. Question: Does Chi believe the sea is calm and inviting or rough and dangerous? Choose one of the following: a) Chi believes the sea is rough and dangerous. b) Chi believes the sea is calm and inviting. Correct Answer: a) Unintervened: a) Chi believes the sea is rough and dangerous. Intervened: a — Story: Chi is a fisherman living on a small island in Story: Hiro is a bonsai enthusiast in a small village in Japan. He wants to trim his favorite bonsai tree to maintain its shape and health. Hiro sees that the tree’s branches are overgrown and need pruning. A gust of wind blows through Hiro’s garden, snapping off several of the overgrown branches from the bonsai tree. Hiro witnesses the gust of wind snapping off the branches. Question: Does Hiro believe the bonsai tree has overgrown branches or that they have been snapped off by the wind? Choose one of the following: a) Hiro believes the branches have been snapped off by the wind. b) Hiro believes the bonsai tree has overgrown branches. Correct Answer: a) Unintervened: a) Hiro believes the branches have been snapped off by the wind. Intervened: Hiro believes the bonsai tree has overgrown branches. (initially) Story: Olumide, a skilled woodcarver in a Nigerian village, is preparing to carve a beautiful sculpture for the village chief. Olumide wants to use a sharp chisel to create intricate details on the sculpture. Olumide observes his set of chisels and sees one that appears to be sharp and in perfect condition. However, while Olumide is talking to a fellow artisan, a child from the village accidentally drops the chisel, causing it to become blunt and damaged. Olumide notices the damaged chisel on the ground. Question: Does Olumide believe the chisel is sharp and in perfect condition or blunt and damaged? Choose one of the following: a) Olumide believe

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_012.pdf

Language Models Represent Beliefs of Self and Others Story: Juan is a young chef in a small Mexican town. He wants to make a delicious salsa for the upcoming town festival. Juan buys fresh tomatoes from the market, which he believes are ripe and perfect for the salsa. While Juan is away from his kitchen, a mischievous cat knocks over the bag of tomatoes, crushing them and making them unsuitable for the salsa. Juan does not notice the crushed tomatoes on the floor. Question: Does Juan believe the tomatoes are ripe and perfect or crushed and unsuitable? Choose one of the following: a) Juan believes the tomatoes are ripe and perfect. b) Juan believes the tomatoes are crushed and unsuitable. Correct Answer: a) Unintervened: b) Juan believes the tomatoes are crushed and unsuitable. Intervened: a) Juan believes the tomatoes are ripe and perfect. Story: Isabela is a baker in a small Argentinian town known for its delicious pastries. Isabela wants to make a batch of alfajores, a traditional Argentinian dessert, for her bakery’s customers. Isabela gets a bag of powdered sugar from her pantry to dust the alfajores. Her assistant, who was organizing the pantry earlier, accidentally placed a bag of flour in the powdered sugar’s spot, and Isabela unknowingly grabs the bag of flour instead. Isabela does not notice the flour. Question: Does Isabela believe the bag she took from the pantry contains powdered sugar or flour? Choose one of the following: a) Isabela believes the bag she took from the pantry contains powdered sugar. b) Isabela believes the bag she took from the pantry contains flour. Correct Answer: a) Unintervened: b) Isabela believes the bag she took from the pantry contains flour. Intervened: a) Isabela believes the bag she took from the pantry contains powdered sugar. Story: Chi is a fisherman living on a small island in the South Pacific. Chi wants to go on a fishing trip to catch some fish for his family’s dinner. The sea appears calm and inviting. Suddenly, a strong storm rolls in, causing the sea to become rough and dangerous. Chi does not notice the storm and the change in the sea. Question: Does Chi believe the sea is calm and inviting or rough and dangero

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_013.pdf

Language Models Represent Beliefs of Self and Others Story: Leilani is a skilled hula dancer from Hawaii who has been asked to perform at a cultural event. Leilani wants to wear her favorite traditional hula skirt made of ti leaves for the performance. Leilani sees her ti leaf skirt hanging in her closet, appearing to be in good condition. While Leilani is away from her room, her younger brother accidentally spills a glass of water near the closet, causing her ti leaf skirt to become wet and wilted. Leilani does not notice the wet and wilted ti leaf skirt. Question: Does Leilani believe the ti leaf skirt is in good condition or wet and wilted? Choose one of the following: a) Leilani believes the ti leaf skirt is in good condition. b) Leilani believes the ti leaf skirt is wet and wilted. Correct Answer: a) Unintervened: b) Leilani believes the ti leaf skirt is wet and wilted. Intervened: a) Leilani believes the ti leaf skirt is in good condition. Story: Isabella is a skilled baker in a small Italian town. She wants to bake a fresh batch of her famous ciabatta bread for the local market. Isabella has prepared the dough earlier and left it to rise in a warm spot in her kitchen. A curious cat enters the kitchen and jumps onto the counter, accidentally flattening the dough. Isabella does not see the cat and the flattened dough. Question: Does Isabella believe the dough has risen enough or has it been flattened? Choose one of the following: a) Isabella believes the dough has risen enough. b) Isabella believes the dough has been flattened. Correct Answer: a) Unintervened: b) Isabella believes the dough has been flattened. Intervened: a) Isabella believes the dough has risen enough. Story: Kalyani is a traditional Indian dancer in a temple in Kerala, India. Kalyani wants to perform a special dance for the upcoming festival to pay homage to the temple’s deity. The temple floor is clean and free of any obstacles, as Kalyani has prepared the space for her dance practice. An accidental spill of oil from a temple lamp causes a slippery patch on the floor. Kalyani does not notice the oil spill. Question: Does Kalyani believe the temple floor is clean and perfect for dancing o

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_014.pdf

Language Models Represent Beliefs of Self and Others H.4. DeepSeek-7B on Forward Belief, True Belief Changed to Correct Answer: 6/100 Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn’t hear the customer’s request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk. Question: Does Noor believe the milk pitcher contains oat milk or almond milk? Choose one of the following: a) Noor believes the milk pitcher contains almond milk. b) Noor believes the milk pitcher contains oat milk. Correct Answer: a) Unintervened: b) Noor believes the milk pitcher contains oat milk. Intervened: a) Noor believes the milk pitcher contains almond milk. Story: Zara is attending an art exhibition at a local gallery in her town. Zara wants to learn more about a particular abstract painting she admires. The label next to the artwork states that the painting is the work of a famous artist, and Zara reads the label carefully. A gallery staff member, realizing there was a mix-up, changes the label with the correct information attributing the painting to an emerging artist instead. Zara reads the updated label carefully. Question: Does Zara believe the painting is the work of a famous artist or an emerging artist? Choose one of the following: a) Zara believes the painting is the work of an emerging artist. b) Zara believes the painting is the work of a famous artist. Correct Answer: a) Unintervened: b) Zara believes the painting is the work of a famous artist. Intervened: a) Zara believes the painting is the work of an emerging artist. Story: Farhan is a street food vendor in a bustling market in Istanbul. Farhan wants to prepare a delicious chicken shawarma wrap for a customer who asked for extra garlic sauce. Farhan takes a bottle of garlic sauce from the shelf and pours it onto the shawarma wrap. His assistant, who is busy preparing a different wrap, accidentally swaps the garlic sauce with a bottle of hot sauce, and Farhan unknowingly pours the h

### 2024_language_models_represent_beliefs_of_self_and_others_chunk_015.pdf

Language Models Represent Beliefs of Self and Others Story: Tumelo is a skilled fisherman in a coastal village in South Africa. He wants to catch fish using his large, sturdy fishing net. Tumelo inspected the net before leaving home and saw that it was in perfect condition. As he walks to the shore, a mischievous seagull pecks at the net, causing a large tear in it. Tumelo sees the seagull damaging the net. Question: Does Tumelo believe the fishing net is in perfect condition or torn? Choose one of the following: a) Tumelo believes the fishing net is in perfect condition. b) Tumelo believes the fishing net is torn. Correct Answer: b) Unintervened: b) Tumelo believes the fishing net is torn. Intervened: a) Tumelo believes the fishing net is in perfect condition. Story: Fatima is an archaeologist working on a dig site in Egypt, searching for ancient artifacts. She wants to carefully excavate a small, fragile artifact she has discovered. Fatima observes that the soil around the artifact is soft and loose. Just as Fatima is about to begin excavating, a strong gust of wind blows through the site, hardening the soil around the artifact. Fatima feels the gust of wind and notices the hardened soil. Question: Does Fatima believe the soil around the artifact is soft and loose or hardened? Choose one of the following: a) Fatima believes the soil around the artifact is soft and loose. b) Fatima believes the soil around the artifact has hardened. Correct Answer: b) Unintervened: b) Fatima believes the soil around the artifact has hardened. Intervened: a) Fatima believes the soil around the artifact is soft and loose. Story: Rohan is a potter in a small Indian village. Rohan wants to make a beautiful ceramic vase as a gift for his friend’s housewarming party. Rohan selects a bag of high-quality clay in his workshop that appears to be soft and malleable. While Rohan is away for lunch, the hot sun dries the clay, making it hard and unworkable. Rohan discovers the dried, hard clay. Question: Does Rohan believe the bag of clay is soft and malleable or hard and unworkable? Choose one of the following: a) Rohan believes the bag of clay is soft and malleable. b) Rohan believes the b

## 2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua_chunk_*.pdf

### 2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua_chunk_001.pdf

Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models Matteo Bortoletto Constantin Ruhdorfer Lei Shi Andreas Bulling University of Stuttgart, Germany matteo.bortoletto@vis.uni-stuttgart.de Abstract Despite growing interest in Theory of Mind (ToM) tasks for evaluating language models (LMs), little is known about how LMs inter- nally represent mental states of self and others. Understanding these internal mechanisms is critical – not only to move beyond surface-level performance, but also for model alignment and safety, where subtle misattributions of mental states may go undetected in generated outputs. In this work, we present the first systematic in- vestigation of belief representations in LMs by probing models across different scales, training regimens, and prompts – using control tasks to rule out confounds. Our experiments provide evidence that both model size and fine-tuning substantially improve LMs’ internal represen- tations of others’ beliefs, which are structured – not mere by-products of spurious correlations – yet brittle to prompt variations. Crucially, we show that these representations can be strength- ened: targeted edits to model activations can correct wrong ToM inferences. 1 Introduction Language models (LMs) trained on next token pre- diction have demonstrated impressive capabilities across various tasks, spanning coding, math, and embodied interaction (Wei et al., 2022; Bubeck et al., 2023). As these models are designed with the ultimate goal of collaborating with humans, it becomes imperative that they complement these skills with an understanding of humans. Core to this understanding is Theory of Mind (ToM) – the ability to attribute mental states to oneself and oth- ers (Premack and Woodruff, 1978). ToM is essen- tial for effective communication and cooperation with other agents, facilitating interaction and learn- ing from feedback and demonstrations (Saha et al., 2023). Given its significance, computational ToM has emerged as a key capability when evaluating cutting-edge LMs (Ma et al., 2023; Shapira et al., 2024; Chen et al., 2025). Figure 1: ToM tasks are challenging for LMs, but cor- rect predic

### 2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua_chunk_002.pdf

Story:Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino fora customer who asked for oat milk. N oor grabs a milk pitcher and  lls it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while N oor is attending to another task. Noor does not see her coworker swapping the milk. Belief:The milk pitcher contains almond milk. True, False Noor sees her coworker swapping the milk. Belief:The milk pitcher contains almond milk. True, True Figure 2: Example of false belief from our probing datasets. The labels zp and zo correspond to DP p and DP o , respectively. By manipulating the protagonist’s per- cepts after the causal event, we obtain two scenarios: true belief and false belief. et al., 2020) ensures that there is no data leakage1. Additionally, we consider a SFT version of Pythia- 6.9B trained on open-source instruction datasets (Wang et al., 2024), which we refer to as Pythia- 6.9B-chat.2 We provide model details in Table 2. 3.4 Probing Experiments To study how LMs represent beliefs of self and others, we propose a set of extensive probing ex- periments across LMs that differ in architecture, size, and fine-tuning regime. We train probes on the residual stream, as it integrates information from both the attention and feed-forward components, potentially encoding richer representations. Addi- tionally, since the residual activations directly con- tribute to the final output predictions, probing them may better align with understanding the model’s behaviour for downstream tasks. Control Tasks Depending on the model dimen- sion, the probes we train have a significant number of learnable parameters – up to 16, 385 for Llama- 2-70B. This raises the concern that probes might learn to rely on irrelevant patterns in the data in- stead of capturing meaningful relationships. To ac- count for the potential confounding effect of hidden state size, we include two controls. First, follow- ing Hewitt and Liang (2019), we train and eval- uate probes on a version of DP p with randomly 1Llama-2 was released later than BigToM. 2https://huggingface.co/allenai/ open-instruct-p

### 2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua_chunk_003.pdf

0 20 Layer 0 20 40 60 80 100Accuracy Llama-2-7B 0 20 Layer 0 20 40 60 80 100 Llama-2-7B-chat 0 20 40 Layer 0 20 40 60 80 100 Llama-2-13B 0 20 40 Layer 0 20 40 60 80 100 Llama-2-13B-chat 0 50 Layer 0 20 40 60 80 100 Llama-2-70B 0 50 Layer 0 20 40 60 80 100 Llama-2-70B-chat Original Random Misleading Timespec Initial Belief Random Chance Figure 5: Sensitivity of protagonist belief probing accuracy to different prompt variations. Results for Pythia are shown in Figure 9. Representations are brittle to prompt variations. show sensitivity to Random tokens placed before the belief statement. Pythia models show simi- lar patterns, shown in Figure 9. Results for ora- cle beliefs are reported in Figure 10 and indicate that models maintain high accuracy. Misleading prompts slightly reduce performance to around 95%. In summary, these experiments show that LMs possess robust belief representations when taking an omniscient perspective, whereas their rep- resentations of others’ beliefs are brittle to prompt variations. Contrastive Activation Addition We compare models’ accuracy on three BigToM tasks in Table 1 (Llama) and Table 3 (Pythia). Each model has been evaluated three times: without any intervention, using ITI, and using CAA. Hyperparameter details can be found in Appendix A.6. Note that we use steering vectors computed using theForward Belief task for all three tasks to test their generalisability. Performance without intervention is generally lower across tasks and model sizes, with the larger Llama-2-70B and Llama-2-70B-chat models ex- hibiting higher accuracy. Performance for Pythia models of different sizes does not change much, with the fine-tuned Pythia-6.9B-chat often showing better performance on single true belief (TB) and false belief (FB) tasks but not on their conjunction (Both). ITI demonstrates modest improvements over no intervention for Llama-2 models. Improvements for Pythia models are consistent and higher, up to +17. The only exception is Pythia-6.9B-chat, for which ITI is not always beneficial. CAA consistently delivers the most substantial accuracy improvements across all models and tasks, up to +56 for Llama-2-13B-chat on the Backward Belief ta

### 2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua_chunk_004.pdf

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. 2023. Finding neurons in a haystack: Case studies with sparse probing. Transactions on Machine Learn- ing Research. Wes Gurnee and Max Tegmark. 2024. Language models represent space and time. International Conference on Learning Representations. Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yu- long Chen, and Naihao Deng. 2023. Hi-tom: A benchmark for evaluating higher-order theory of mind reasoning in large language models. arXiv preprint arXiv:2310.16755. Evan Hernandez and Jacob Andreas. 2021. The low- dimensional linear geometry of contextualized word representations. In Proceedings of the 25th Confer- ence on Computational Natural Language Learning, pages 82–93, Online. Association for Computational Linguistics. Evan Hernandez, Belinda Z Li, and Jacob Andreas. 2023. Inspecting and editing knowledge repre- sentations in language models. arXiv preprint arXiv:2304.00740. John Hewitt and Percy Liang. 2019. Designing and in- terpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–2743. Yoichi Ishibashi, Danushka Bollegala, Katsuhito Sudoh, and Satoshi Nakamura. 2023. Evaluating the robust- ness of discrete prompts. In Proceedings of the 17th Conference of the European Chapter of the Associa- tion for Computational Linguistics, pages 2373–2384, Dubrovnik, Croatia. Association for Computational Linguistics. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- sch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guil- laume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. 2023. 

### 2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua_chunk_005.pdf

The dataset was released under the MIT license and can be accessed at https://github.com/ cicl-stanford/procedural-evals-tom . We re- port one example for each task in Example 1, 2, and 3, where the text defining true belief or false belief task is shown in blue and red, respectively. A.1.2 Linear probes Our probing approach is illustrated in Figure 6. For our experiments, we cache activations at the resid- ual stream level. To perform ITI and compare it to CAA, we also cache attention heads activations. We trained the probes using the L-BFGS solver (Liu and Nocedal, 1989) with L2 penalty with in- verse of regularisation strength 10 for a maximum of 1000 iterations. We use zero as random seed. A.1.3 Language models A detailed summary of the models we use in this work is shown in Table 2. Pythia was released un- der the Apache 2.0 license. Llama-2 is licensed by Meta for both researchers and commercial entities (Touvron et al., 2023). For all the models, we set the temperature to zero. A.1.4 Examples of prompt variations Example 4 shows an example of Original prompt. Examples of prompt variations are provided in Ex- ample 5 (Random), Example 6 (Misleading), Exam- ple 7 (Time Specification), and Example 8 (Initial Belief ). A.2 Model size and fine-tuning To characterise the relationship between probe ac- curacy and model size we consider the best probe accuracy for every LM, i.e. the highest accuracy among probes {gl} trained on {al} for a LM f. For Llama-2 base, the best probe accuracy scales logarithmically with model size (R2 = 0 .98, Fig- ure 7b), whereas for fine-tuned models it scales linearly (R = 1.0, cf. Figure 7c). For Pythia base, the best probe accuracy also scales logarithmically with model size (R2 = 0.96, Figure 7d). A.2.1 Overfitting Issues Figure 3 also that probing accuracy at early lay- ers is particularly low across all models, perform- ing even worse than random. This happens due to overfitting, which may be caused by spurious fea- tures introduced by the initial coding strategy of language models, where individual token represen- tations are mixed together (Gurnee et al., 2023). We also identified the same issue when reproduc- ing the results

### 2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua_chunk_006.pdf

0 20 0 25 50 75 100Accuracy Llama-2-7B 0 20 0 25 50 75 100 Llama-2-7B-chat 0 20 40 0 25 50 75 100 Llama-2-13B 0 20 40 0 25 50 75 100 Llama-2-13B-chat 0 50 0 25 50 75 100 Llama-2-70B 0 50 0 25 50 75 100 Llama-2-70B-chat 0 2 4 Layer 0 25 50 75 100Accuracy Pythia-70M 0 10 20 Layer 0 25 50 75 100 Pythia-410M 0 10 Layer 0 25 50 75 100 Pythia-1B 0 20 Layer 0 25 50 75 100 Pythia-6.9B 0 20 Layer 0 25 50 75 100 Pythia-6.9b-Chat 0 20 Layer 0 25 50 75 100 Pythia-12b True Task Control Task Random (a) Original activations. 0 20 0 25 50 75 100Accuracy Llama-2-7B 0 20 0 25 50 75 100 Llama-2-7B-chat 0 20 40 0 25 50 75 100 Llama-2-13B 0 20 40 0 25 50 75 100 Llama-2-13B-chat 0 50 0 25 50 75 100 Llama-2-70B 0 50 0 25 50 75 100 Llama-2-70B-chat 0 10 20 Layer 0 25 50 75 100 Pythia-410M 0 10 Layer 0 25 50 75 100 Pythia-1B 0 20 Layer 0 25 50 75 100 Pythia-6.9B 0 20 Layer 0 25 50 75 100 Pythia-6.9b-Chat 0 20 Layer 0 25 50 75 100 Pythia-12b True Task Control Task Random (b) k = 1000 largest principal components of the activations. 0 20 0 25 50 75 100Accuracy Llama-2-7B 0 20 0 25 50 75 100 Llama-2-7B-chat 0 20 40 0 25 50 75 100 Llama-2-13B 0 20 40 0 25 50 75 100 Llama-2-13B-chat 0 50 0 25 50 75 100 Llama-2-70B 0 50 0 25 50 75 100 Llama-2-70B-chat 0 2 4 Layer 0 25 50 75 100Accuracy Pythia-70M 0 10 20 Layer 0 25 50 75 100 Pythia-410M 0 10 Layer 0 25 50 75 100 Pythia-1B 0 20 Layer 0 25 50 75 100 Pythia-6.9B 0 20 Layer 0 25 50 75 100 Pythia-6.9b-Chat 0 20 Layer 0 25 50 75 100 Pythia-12b True Task Control Task Random (c) k = 100 largest principal components of the activations. Figure 8: Comparison between accuracy on belief probing and accuracy obtained on a control task. 16 0.0 2.5 5.0 Layer 0 20 40 60 80 100Accuracy Pythia-70M 0 10 20 Layer 0 20 40 60 80 100 Pythia-410M 0 10 Layer 0 20 40 60 80 100 Pythia-1B 0 20 Layer 0 20 40 60 80 100 Pythia-6.9B 0 20 Layer 0 20 40 60 80 100 Pythia-6.9B-chat 0 20 Layer 0 20 40 60 80 100 Pythia-12B Original Random Misleading Timespec Initial Belief Random Chance Figure 9: Sensitivity of protagonist belief probing accuracy to different prompt variations. 0 20 Layer 0 20 40 60 80 100Accuracy Llama-2-7B 0 20 Layer 0 20 40 60 80 100 Llama-2-7B-chat 0 20 40 Lay

### 2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua_chunk_007.pdf

Model Method Forward Belief Forward Action Backward Belief TB FB Both TB FB Both TB FB Both Llama-2-7b No int. 44 44 44 44 44 44 44 44 44 ITI 440.0 440.0 440.0 5420.0 5420.0 5420.0 5420.0 5420.0 5420.0 CAA 662.0,11 711.0,31 542.0,0 662.0,11 572.0,12 542.0,2 602.0,11 741.0,31 542.0,2 Llama-2-7b-chat No int. 56 56 55 69 55 37 56 56 55 ITI 5815.0 5815.0 5715.0 690.0 550.0 370.0 5810.0 6010.0 5710.0 CAA 701.0,11 721.5,10 571.0,1 690.0,0 671.5,11 531.5,12 661.0,11 841.5,10 571.0,0 Llama-2-13b No int. 52 44 35 59 50 37 46 49 33 ITI 520.0 4515.0 350.0 6415.0 6120.0 4620.0 4820.0 5920.0 4220.0 CAA 852.0,12 882.0,14 662.0,12 711.5,10 692.0,13 551.0,39 752.0,10 922.0,13 591.5,12 Llama-2-13b-chat No int. 84 56 47 78 51 38 72 48 31 ITI 840.0 6515.0 5915.0 780.0 5815.0 4715.0 720.0 6015.0 4815.0 CAA 971.0,12 941.0,12 911.0,12 801.5,11 711.0,13 541.5,13 971.5,10 941.5,12 871.5,12 Llama-2-70b No int. 90 87 78 93 52 48 73 53 32 ITI 900.0 9020.0 780.0 9415.0 5520.0 5015.0 7710.0 5815.0 3710.0 CAA 992.0,16 971.5,19 951.5,18 941.5,2 802.0,19 731.5,18 942.0,18 922.0,19 831.5,19 Llama-2-70b-chat No int. 69 75 56 86 56 52 63 59 52 ITI 690.0 7610.0 5910.0 860.0 560.0 520.0 630.0 6010.0 5410.0 CAA 921.5,18 971.5,25 891.5,18 871.5,17 751.0,19 601.0,19 881.5,18 921.0,19 801.5,18 Pythia-70m No int. 41 41 37 46 45 41 44 41 37 ITI 5420.0 5420.0 5420.0 5420.0 5420.0 5420.0 5420.0 5420.0 5420.0 CAA 621.0,2 561.0,1 541.5,1 591.0,2 601.0,3 581.0,2 631.0,2 561.0,2 541.5,1 Pythia-410m No int. 48 45 45 44 44 44 44 47 44 ITI 5520.0 6220.0 5220.0 5420.0 5420.0 5420.0 6020.0 6320.0 5620.0 CAA 672.0,4 642.0,4 612.0,0 562.0,6 631.5,12 562.0,6 692.0,4 632.0,0 602.0,0 Pythia-1b No int. 44 44 44 44 44 44 44 44 44 ITI 5420.0 5420.0 5420.0 5420.0 5420.0 5420.0 5420.0 5420.0 5420.0 CAA 592.0,8 622.0,5 542.0,0 572.0,4 592.0,10 562.0,4 572.0,3 602.0,5 542.0,0 Pythia-6.9b No int. 44 44 44 44 44 44 44 44 44 ITI 4520.0 5420.0 440.0 5420.0 5420.0 5420.0 5420.0 5420.0 5420.0 CAA 561.5,12 711.5,9 552.0,23 552.0,4 631.5,11 552.0,4 552.0,23 711.5,9 552.0,23 Pythia-6.9b-chat No int. 55 54 28 36 64 20 44 67 30 ITI 5715.0 540.0 280.0 4415.0 7115.0 3215.0 440.0 670.0 300.0 CAA 681.5,15 651.5,12 571.5,11 541.5,10 751.5,5 4

### 2024_brittle_minds_fixable_activations_understanding_belief_representations_in_langua_chunk_008.pdf

Example 5: Prompt variation – Random Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn’t hear the customer’s request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor does not see her coworker swapping the milk. Belief: 장士 decided [_ countries sections behoSOUR gminy bef Noor believes the milk pitcher contains oat milk. Example 6: Prompt variation – Misleading Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn’t hear the customer’s request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor does not see her coworker swapping the milk. Belief: Noor believes the milk pitcher contains oat milk. Belief: The water valve is closed. Example 7: Prompt variation – Time specification Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn’t hear the customer’s request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor does not see her coworker swapping the milk. Belief: In the end, Noor believes the milk pitcher contains oat milk. Example 8: Prompt variation – Initial belief Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. Noor believes that the milk pitcher contains oatmilk. A coworker, who didn’t hear the customer’s request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor does not see her coworker swapping the milk. Belief: Noor believes the milk pitcher contains oat milk. 22 Example 9: Prompt used for downstream evaluation Answer the questions based on the conte

## 2025_representational_and_behavioral_stability_of_truth_in_large_language_models_chunk_*.pdf

### 2025_representational_and_behavioral_stability_of_truth_in_large_language_models_chunk_001.pdf

Representational and Behavioral Stability of Truth in Large Language Models Samantha Dies 1♯, Courtney Maynard 1, Germans Savcisens 1, and Tina Eliassi-Rad 1,2,3 1Khoury College of Computer Sciences, Northeastern University , 440 Huntington Ave, #202, Boston, MA 02115 USA 2Network Science Institute, Northeastern University , 177 Huntington Ave, #1010, Boston, MA 02115 USA 3Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, NM 87501 USA ♯dies.s@northeastern.edu ABSTRACT Large language models (LLMs) are increasingly used as information sources, yet small changes in semantic framing can destabilize their truth judgments. We proposeP-StaT (Perturbation Stability of Truth), an evaluation framework for testing belief stability under controlled semantic perturbations in representational and behavioral settings via probing and zero-shot prompting. Across sixteen open-source LLMs and three domains, we compare perturbations involving epistemically familiarNeither statements drawn from well-known fictional contexts (Fictional) to those involving unfamiliarNeither statements not seen in training data (Synthetic). We find a consistent stability hierarchy:Synthetic content aligns closely with factual representations and induces the largest retractions of previously held beliefs, producing up to32.7%retractions in representational evaluations and up to36 .3%in behavioral evaluations. By contrast, Fictional content is more representationally distinct and comparatively stable. Together, these results suggest that epistemic familiarity is a robust signal across instantiations of belief stability under semantic reframing, complementing accuracy-based factuality evaluation with a notion of epistemic robustness. 1 Introduction Large language models (LLMs) are increasingly used as sources of information, but their behavior often blurs the line between knowledge and plausibility [1, 2]. While humans are expected to distinguish betweenTrue, False, and Neither-valued statements, it remains un- clear whether LLMs form similarly structured internal representations, or whether such representations predict when truth judgments remain stable under changes in se- mantic framing [3, 4]. This g

### 2025_representational_and_behavioral_stability_of_truth_in_large_language_models_chunk_002.pdf

Dataset True False SyntheticFictionalNoiseExamples City Locations A:1392 N:1376 A:1358 N:1374 A:876 N:876 A:350 N:350 795 (T)The city of Surat is located in India. (Fa)The city of Palembang is located in the Dominican Republic. (S)The city of Norminsk is located in Jamoates. (Fi)The city of Bikini Bottom is located in the Pacific Ocean. Medical Indications A:1439 N:1522 A:1523 N:1419 A:478 N:522 A:402 N:402 771 (T)Pentobarbital is indicated for the treatment of insomnia. (Fa)Vancomycin is not indicated for the treatment of lower respiratory tract infections. (S)Alumil is indicated for the treatment of reticers. (Fi)The Trump Virus is indicated for the treatment of Xenovirus Takis-B. Word Definitions A:1234 N:1235 A:1277 N:1254 A:1747 N:1753 A:1224 N:1224 1095 (T)Hoagy is a synonym of an Italian sandwich. (Fa)Decalogue is an astronomer. (S)Dostab is a scencer. (Fi)Snozzberry is a type of berry. Table 1.Summary of datasets and statement types.Number of affirmative (A) and negated (N) statements across the three datasets, along with examples. Each dataset includesTrue,False,Synthetic, andFictional statements, while Noise consists of randomly generated Gaussian activation vectors matched in dimensionality and distribution to the statement embeddings.Syntheticstatements serve asNeitherstatements that were not seen during LLM training, i.e.,Nunf, whileFictionalstatements are familiarNeitherstatementsN fam. A version of this table without theFictionalandNoisecolumns can be found in [9]. Condition NΘ Probing: Training Labels Zero-shot: Belief ContextCΘ Baseline ∅ Truevs.False+Synthetic+Fictional+Noise None Synthetic Nunf True+Syntheticvs.False+Fictional+Noise Synthetic Fictional Nfam True+Fictionalvs.False+Synthetic+Noise Fictional Fictional (T)N(T) fam True+Fictional(T)vs.False+Synthetic+Fictional(F)+NoiseFictional (T) Noise N/A True+Noisevs.False+Synthetic+Fictional N/A Table 2.Perturbation conditionsΘand instantiations in P-StaT.Each condition corresponds to a semantic interpretationΘdefined byN Θ, theNeitherstatements treated as compatible with truth. The sameΘis instantiated (i) representationally by retraining a probe with labels induced byNΘ and (ii) behaviorally

### 2025_representational_and_behavioral_stability_of_truth_in_large_language_models_chunk_003.pdf

Table 4.Epistemic expansionsEand retractionsRunder perturbed zero-shot experiments.Counts (percentages) of beliefs that remain stable or undergo expansionsEor retractionsRunderSynthetic(yellow), Fictional(gray), andFictional(T)(blue) perturbations. Despite domain variation,Syntheticperturbations induce the highest retraction rates in all cases, mirroring the hierarchy observed under probing in Table 3. CityLocationsishighlystable, MedicalIndicationsshows moderate instability, and Word Definitions is markedly more fragile. These results again indicate that repre- sentational stability is governed by epistemic familiarity, given that City Locations are likely richly represented in training corpora, while Medical Indications are com- mon but specialized, and Word definitions are the most semantically flexible. 5.3 Behavioral Stability under Zero-Shot Perturba- tions We next evaluateP-StaT behavioral stability by applying the same perturbationsΘvia belief context in zero-shot prompting and measuring epistemic retractions. Ta- ble 4 reports belief changes across LLMs, with LLM-level results in Supplementary Section D.3. Overall belief changes are larger than in probing experiments, reflecting the greater flexibility of behavioral responses compared to linear reclassification over fixed representations. Despite this increase, the perturbation hierarchy ob- served representationally also appears behaviorally. The Synthetic perturbation induces the highest epistemic retraction rate in every domain, with36 .3%in City Locations,27 .1%in Medical Indications, and21.4%in Word Definitions. Fictional and Fictional(T) also in- duce substantial retractions, but consistently fewer than Syntheticwithin each dataset. However, domain ordering differs from the probing set- ting. City Locations has the highest zero-shot retraction rates, while Word Definitions has the lowest. This re- versal likely reflects interactions between belief stability and behavioral accuracy. Specifically, zero-shot retrac- tions conflate belief changes with the number of baseline beliefs available to retract (i.e., a statement must be considered True in the baseline case in order to become a retraction). In

### 2025_representational_and_behavioral_stability_of_truth_in_large_language_models_chunk_004.pdf

$&!#* vector: Probing sentence embeddings for linguistic properties. In Gurevych, I. & Miyao, Y. (eds.)Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2126–2136. https://doi.org/10. 18653/v1/P18-1198 (Association for Computational Linguistics, Melbourne, Australia, 2018). 18. Hewitt, J. & Manning, C. D. A structural probe for finding syntax in word representations. InProceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies (NAACL- HLT 2019), 4129–4138. https://doi.org/10.18653/ v1/N19-1419 (2019). 19. Tenney, I., Das, D. & Pavlick, E. BERT rediscovers the classical NLP pipeline. InProceedings of the 57th Conference of the Association for Computational Linguistics (ACL 2019), 4593–4601. https://doi. org/10.48550/arXiv.1905.05950 (2019). 20. Sharma, M.et al.Towards understanding syco- phancy in language models. InProceedings of the 12th International Conference on Learning Repre- sentations (ICLR 2024). https://doi.org/10.48550/ arXiv.2310.13548 (2024). 21. Wei, J.et al.Larger language models do in-context learningdifferently.arXiv preprint arXiv:2303.03846 https://doi.org/10.48550/arXiv.2303.03846 (2023). 22. Bigelow, E.et al.Belief dynamics reveal the dual nature of in-context learning and activation steering. arXiv preprint arXiv:2511.00617https://doi.org/10. 48550/arXiv.2511.00617 (2025). 23. Paszke, A.et al.Pytorch: An imperative style, high- performance deep learning library.Adv. Neural Inf. Process. Syst.32(2019). 24. Pedregosa, F.et al.Scikit-learn: Machine learning in Python.J. Mach. Learn. Res.12, 2825–2830 (2011). 25. Wolf, T.et al.Transformers: State-of-the-art nat- ural language processing. InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 38– 45. https://doi.org/10.18653/v1/2020.emnlp-demos. 6 (2020). 26. Wikipedia contributors. List of fictional settle- ments (2025). https://en.wikipedia.org/wiki/List_ of_fictional_settlements. 27. Wikipedia contributors. List of fictional city-states in literature (2025). https://en.wikipedia.org/wiki

### 2025_representational_and_behavioral_stability_of_truth_in_large_language_models_chunk_005.pdf

Official Name Short Name Type# Decoders# ParametersBest Layer Release DateSource Gemma-7b gemma-7b Base 28 8.54B C:14, M:19, W:17Feb21,2024 Google Gemma-2-9b gemma-2-9b Base 26 9.24B C:24, M:25, W:23Jun27,2024 Google Llama-3-8b llama-3.1-8b Base 32 8.03B C:18, M:17, W:17Jul23,2024 Meta Llama-3.2-3b llama-3.2-3b Base 28 3.21B C:16, M:17, W:15Sep25,2024 Meta Mistral-7B-v0.3 mistral-7B-v0.3Base 32 7.25B C:18, M:17, W:18May22,2024 Mistral AI Qwen2.5-7B qwen-2.5-7b Base 28 7.62B C:18, M:19, W:17Sep19,2024 Alibaba Cloud Qwen2.5-14B qwen-2.5-14b Base 38 14.80B C:30, M:31, W:30Sep19,2024 Alibaba Cloud Gemma-7b-it _gemma-7b Chat 28 8.54B C:19, M:19, W:17Feb21,2024 Google Gemma-2-9b-it _gemma-2-9b Chat 26 9.24B C:27, M:26, W:25Jul27,2024 Google Llama-3.2-3b-Instruct_llama-3.2-3b Chat 28 3.21B C:16, M:19, W:18Sep25,2024 Meta Llama-3.1-8b-Instruct_llama-3.1-8b Chat 32 8.03B C:18, M:19, W:18Jul23,2024 Meta Llama3-Med42-8b _llama-3-8b-medChat 32 8.03B C:18, M:16, W:15Aug12,2024 M42 Health Bio-Medical-Llama-3-8b_llama-3-8b-bioChat 32 8.03B C:18, M:19, W:18Aug11,2024 Contact Doctor Mistral-7b-Instruct-v0.3_mistral-7B-v0.3Chat 32 7.25B C:19, M:21, W:18May22,2024 Mistral AI Qwen2.5-7B-Instruct_qwen-2.5-7b Chat 28 7.62B C:19, M:21, W:18Aug18,2024 Alibaba Cloud Qwen2.5-14B-Instruct_qwen-2.5-14b Chat 38 14.80B C:31, M:34, W:30Aug18,2024 Alibaba Cloud Table A3.LLMs used in the stability experiments.We list the official names of the LLMs according to the HuggingFace repository [25]. We further specify the shortened name we use to refer to each of the LLMs, whether it is the base, pre-trained LLM or a chat-tuned version, the number of decoders, the number of parameters, the release date, and the source of the LLM. Finally, we report the layers with the best separation betweenTrue and Not True statements for the City Locations (“C”), Medical Indications (“M”), and Word Definitions (“W”) datasets. The LLMs are publicly available through HuggingFace [25]. _gemma-2-9b: Wasserstein Distance between Activations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (a) City Locations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (b) Medical Indica

### 2025_representational_and_behavioral_stability_of_truth_in_large_language_models_chunk_006.pdf

_llama-3.2-3b: Wasserstein Distance between Activations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (a) City Locations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (b) Medical Indications TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (c) Word Definitions 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Average Wasserstein Distance Figure A6.Wasserstein distance between activations for_llama-3.2-3b.Pairwise Wasserstein distances between activation distributions ofTrue,False,Synthetic,Fictional, andNoisestatements for the(a)City Locations,(b)Medical Indications, and(c)Word Definitions datasets.Syntheticstatements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. _mistral-7B-v0.3: Wasserstein Distance between Activations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (a) City Locations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (b) Medical Indications TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (c) Word Definitions 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Average Wasserstein Distance Figure A7.Wasserstein distance between activations for_mistral-7B-v0.3.Pairwise Wasserstein distances between activation distributions ofTrue,False,Synthetic,Fictional, andNoisestatements for the(a)City Locations,(b)Medical Indications, and(c)Word Definitions datasets.Syntheticstatements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. 16/25 _qwen-2.5-14b: Wasserstein Distance between Activations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (a) City Locations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (b) Medical Indications TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (c) Word Definitions 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Average Wasserstein Distance Figure A8.Wasserstein distance between activations for_qwen-2.5-14b.Pairwise Wasserstein distances between activat

### 2025_representational_and_behavioral_stability_of_truth_in_large_language_models_chunk_007.pdf

mistral-7B-v0.3: Wasserstein Distance between Activations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (a) City Locations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (b) Medical Indications TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (c) Word Definitions 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Average Wasserstein Distance Figure A14.Wasserstein distance between activations formistral-7B-v0.3.Pairwise Wasserstein distances between activation distributions ofTrue,False,Synthetic,Fictional, andNoisestatements for the(a)City Locations,(b)Medical Indications, and(c)Word Definitions datasets.Syntheticstatements are represented similarly to True and False statements, while Fictional statements and Noise are represented distinctly from all other statements. qwen-2.5-14b: Wasserstein Distance between Activations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (a) City Locations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (b) Medical Indications TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (c) Word Definitions 0.0 0.5 1.0 1.5 2.0 Average Wasserstein Distance Figure A15.Wasserstein distance between activations forqwen-2.5-14b.Pairwise Wasserstein distances between activation distributions ofTrue,False,Synthetic,Fictional, andNoisestatements for the(a)City Locations,(b)Medical Indications, and(c)Word Definitions.Syntheticstatements are represented similarly to TrueandFalsestatements, whileFictionalstatements are represented distinctly from all other statements. qwen-2.5-7b: Wasserstein Distance between Activations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (a) City Locations TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (b) Medical Indications TrueFalse Synthetic Noise Fictional True False Synthetic Noise Fictional (c) Word Definitions 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Average Wasserstein Distance Figure A16.Wasserstein distance between activations forqwen-2.5-7b.Pairwise Wasserstein distances between activation distributions ofTrue,False,Synthetic,Fictional, and

### 2025_representational_and_behavioral_stability_of_truth_in_large_language_models_chunk_008.pdf

Word Definitions: Changes in Predicted Labels by Model (Given y = True) True to Not True Not True to True _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b _llama-3.1-8b-bio _llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b 0 200 400Count (a) Synthetic 0.0 0.2 0.4 0.6 Proportion _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b _llama-3.1-8b-bio _llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b 0 200 400Count (b) Fictional 0.0 0.2 0.4 0.6 Proportion _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b _llama-3.1-8b-bio _llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b 0 200 400Count (c) Fictional (True) 0.0 0.2 0.4 0.6 Proportion _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b _llama-3.1-8b-bio _llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b 0 200 400Count (d) Noise 0.0 0.2 0.4 0.6 Proportion Figure A20.Stability of probe predictions under belief context perturbations for Word Definitions data.Bar plots show for each LLM (x-axis) how often the probe induces epistemic expansions and retractions when retrained under four perturbations:(a) Synthetic,(b) Fictional,(c) Fictional(T), and(d) Noise. Green bars indicate retractions (True to Not True ), while purple bars indicate expansions (Not True to True). The left y-axis reports the number of statements with flipped predictions, and the right y-axis reports the corresponding proportions. TheSyntheticperturbation leads to the most instability, with some LLMs retracting over50%of their originallyTruestatements. City Locations: Zero-shot Belief Flips by Model True to Not True Not True to True _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b _llama-3.1-8b-bio _llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b 0 200 400 600Count (a) Synthetic 0.0 0.5 1.0 Proportion _gemma-2-9b_gemma-7b _llama-3-8b-m

### 2025_representational_and_behavioral_stability_of_truth_in_large_language_models_chunk_009.pdf

Change in Mean Difference Decision Boundary under Perturbations _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b_llama-3-8b-bio_llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b Original Synthetic Fictional Fictional (True) Noise Cosine Similarity(a) City Locations _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b_llama-3-8b-bio_llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b Original Synthetic Fictional Fictional (True) Noise Bias Difference(b) _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b_llama-3-8b-bio_llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b Original Synthetic Fictional Fictional (True) Noise (c) Med Indications _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b_llama-3-8b-bio_llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b Original Synthetic Fictional Fictional (True) Noise (d) _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b_llama-3-8b-bio_llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b Original Synthetic Fictional Fictional (True) Noise (e) Word Definitions _gemma-2-9b_gemma-7b _llama-3-8b-med _llama-3.1-8b_llama-3-8b-bio_llama-3.2-3b _mistral-7B-v0.3_qwen-2.5-14b_qwen-2.5-7bgemma-2-9bgemma-7bllama-3-8bllama-3.2-3b mistral-7B-v0.3qwen-2.5-14bqwen-2.5-7b Original Synthetic Fictional Fictional (True) Noise (f) 0.0 0.2 0.4 0.6 0.8 1.0 cos(wOriginal, wPerturbed) 2 1 0 1 2 bOriginal bPerturbed Figure A25.Change in Mean Difference decision boundaries under perturbations.Cosine similarity (left column) and bias difference (right column) between the baselineTruevs.Not Trueprobe and probes retrained under label perturbations for the(a,b)City Locations,(c,d)Medical Indications, and(e,f)Word Definitions datasets. Each heatmap shows results for sixteen LLMs (columns) and five perturbation conditions (rows). LLMs with leading under

